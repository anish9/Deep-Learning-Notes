{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "424052b2-6578-471f-99d2-59f38e210d63",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange;text-align:center;font-family:courier;font-size:280%\">YOLOv1 from scratch</h1>\n",
    "<p style=\"color:orange;text-align:center;font-family:courier\"> The objective is to understand how single stage object detectors work using YOLOv1 algorithm</p>\n",
    "\n",
    "### Objectives \n",
    "* Understand the theory and building blocks of object detection problem.\n",
    "* Generate a basic understanding why new algorithms are needed to solve complex detection problems.\n",
    "* simplify the pedagogy of explaining computer vision topics.\n",
    "<!-- * Though the code works there are significant drawbacks with yolov1 which has been addressed on YoloV2,YoloV3 -->\n",
    "\n",
    "\n",
    "<p style=\"text-align:center\"><img src=\"assets/yolo.jpeg\" alt=\"yolov1\" width=\"240\"/>\n",
    "<p style=\"text-align:center\"><img src=\"assets/pipeline.png\" alt=\"yolov1_od\" width=\"640\"/>\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c44e017f-0d8a-42db-adb5-91b0fa317f93",
   "metadata": {},
   "source": [
    "### Import dependencies\n",
    "\n",
    "###### Essential links to understand tutorial\n",
    "* <a href=https://www.pyimagesearch.com/2016/11/07/intersection-over-union-iou-for-object-detection/>Read about IoU (Intersection over union)</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25b2ce83-c4d6-42e4-94ab-e852824fedf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np # linear algebra\n",
    "import tensorflow as tf # deep learning\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from  tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras import layers,models\n",
    "from tensorflow.keras.callbacks import *\n",
    "from detect_utils import (iou,xmin_ymin_centre_wh,\n",
    "                          rescale_csv,prepare_csv,\n",
    "                         xw_xmin_ymin,basic_NMS) #object detection utilities\n",
    "\n",
    "from detect_utils import (get_predictions,\n",
    "                          process_predictions,get_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b462742d-f3b8-47bf-bf63-b96e71ba927f",
   "metadata": {},
   "source": [
    "#### Dataset Structure information\n",
    "* In general we use normalized coordinates and below is the sample csv structure.\n",
    "* we have scaled all the image dimensions (464,464,3) before training, therefore coordinates as well. \n",
    "\n",
    "<p style=\"text-align:center\"><img src=\"assets/dataset_csv.png\" alt=\"yolov1_od\" width=\"640\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7882577f-cd1f-4fda-a19b-3210b72adf3a",
   "metadata": {},
   "source": [
    "#### Implementation of Custom DataLoader\n",
    "* <a href=https://www.tensorflow.org/api_docs/python/tf/keras/utils/Sequence>Read about Creating Tensorflow custom dataloader using Sequence API class</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94c41633-6344-48d2-a074-38a04342afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Sequence):\n",
    "    def __init__(self,csv,img_dir,image_size,grid_size,classes,batch_size,aug=False,shuffle=True):\n",
    "        self.csv   = csv\n",
    "        self.img_dir    = img_dir\n",
    "        self.imsize     = image_size\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle    = shuffle\n",
    "        self.grid_size = grid_size\n",
    "        self.classes = classes\n",
    "        self.indices    = list(range(len(list(set(self.csv.image)))))\n",
    "        self.aug = aug\n",
    "        \n",
    "    \n",
    "    def __len__(self):\n",
    "        return int(len(self.indices)/self.batch_size)-1\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "    \n",
    "    def __process__(self,index):\n",
    "        out_grid = np.zeros((self.grid_size,self.grid_size,5+self.classes))\n",
    "        index = index\n",
    "        csvd = self.csv\n",
    "        all_files = list(set(csvd.image))\n",
    "        selected  = csvd[csvd[\"image\"]==all_files[index]]\n",
    "        selected = selected.values\n",
    "        im_tensor = tf.io.decode_png(tf.io.read_file(self.img_dir+selected[0,0]),channels=3)\n",
    "        im_tensor = tf.image.resize(im_tensor,self.imsize)\n",
    "        if self.aug:\n",
    "            p1 = np.random.randint(1,15,1)[0]\n",
    "            p2 = np.random.randint(1,15,1)[0]\n",
    "            if p1 >= 9:\n",
    "                im_tensor=tf.image.random_contrast(im_tensor,0.6,1.6)\n",
    "            if p2 >= 13:\n",
    "                im_tensor=tf.image.random_brightness(im_tensor,0.3)\n",
    "        im_tensor = im_tensor/255.\n",
    "        boxes    = selected[:,2:-2]\n",
    "        labels   = selected[:,1]\n",
    "        labels = tf.cast([labels],tf.uint8)\n",
    "        labels = tf.one_hot(labels,self.classes+1)[0][:,1:]\n",
    "        #first convert from xmin,ymin to center\n",
    "        centres = xmin_ymin_centre_wh(boxes) #[x_c,y_c,width,height]\n",
    "        centres  =tf.concat((centres,labels),axis=1)\n",
    "\n",
    "        #multiply by out_grid scale\n",
    "        for bboxs in centres:\n",
    "            bbox,cls_ = bboxs[:4],bboxs[4:]\n",
    "            cx,cy,wid,hei = bbox \n",
    "            \n",
    "            g_cx,g_cy   = cx*self.grid_size,cy*self.grid_size #convert between 0 to 1\n",
    "            i,j = int(g_cx),int(g_cy)\n",
    "            g_cx,g_cy   = g_cx-int(g_cx),g_cy-int(g_cy) #convert between 0 to 1\n",
    "            g_wid,g_hei = wid*self.grid_size,hei*self.grid_size # convert with respect to predict grid\n",
    "\n",
    "            \n",
    "            if out_grid[i,j,4] ==0:\n",
    "                out_grid[i,j,4] = 1\n",
    "                out_grid[i,j,:4] = [g_cx,g_cy,g_wid,g_hei]\n",
    "                if self.classes == 1:\n",
    "                    out_grid[i,j,5] = 1\n",
    "                else:\n",
    "                    out_grid[i,j,5:] = cls_\n",
    "        return im_tensor,out_grid\n",
    "\n",
    "    \n",
    "    def __getitem__(self,idx):\n",
    "        x_ = []\n",
    "        y_ = []\n",
    "\n",
    "        batch_list = list(range(idx * self.batch_size,(idx + 1)*self.batch_size))\n",
    "        for idx_ in batch_list:\n",
    "            x,y = self.__process__(idx_)\n",
    "            x_.append(x)\n",
    "            y_.append(y)\n",
    "\n",
    "        return tf.cast(x_,tf.float32),tf.cast(y_,tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c4b6fc-dd34-4120-87bc-14b4e85697c1",
   "metadata": {},
   "source": [
    "### Loading Dataset \n",
    "##### setting up paths and train configuration.\n",
    "\n",
    "* we use grid size of 7x7 as per the paper.\n",
    "* we will use pascal dataset just selecting 6 lables out of 20 to fasten the experiment with batch size of 4(GPU constraints).\n",
    "* The exclude_list contains the labels which we will avoid during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0ff4ff8-935e-4394-a643-37787d13d900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Labels : {0: 'aeroplane', 1: 'motorbike', 2: 'dog', 3: 'bicycle', 4: 'car', 5: 'bus'}\n"
     ]
    }
   ],
   "source": [
    "train_csv_path  = \"pascal/464_train_scaled.csv\"\n",
    "train_image_dir = \"pascal/464_train_scaled/\"\n",
    "\n",
    "val_csv_path   = \"pascal/464_valid_scaled.csv\"\n",
    "val_image_dir  = \"pascal/464_valid_scaled/\"\n",
    "\n",
    "exclude_classes=['person','train','horse','tvmonitor',\n",
    "                 'diningtable','cow','sofa','chair', \n",
    "                 'cat', 'bird', 'pottedplant', 'boat', \n",
    "                 'sheep', 'bottle']\n",
    "\n",
    "grid_size = 7\n",
    "# classes = 6\n",
    "batch_size = 4\n",
    "image_height, image_width = 464,464\n",
    "\n",
    "train_csv_file,val_csv_file,classes,label_map = prepare_csv(train_csv_path=train_csv_path,\n",
    "                                                            val_csv_path=val_csv_path,exclude_label=exclude_classes)\n",
    "\n",
    "train_data = DataLoader(train_csv_file,img_dir=train_image_dir,\n",
    "                        image_size=(image_height, image_width),\n",
    "                        grid_size=grid_size,classes=classes,\n",
    "                        batch_size=batch_size,aug=True)\n",
    "\n",
    "\n",
    "val_data   = DataLoader(val_csv_file,img_dir=val_image_dir,\n",
    "                       image_size=(image_height, image_width),\n",
    "                       grid_size=grid_size,classes=classes,\n",
    "                       batch_size=batch_size,aug=False)\n",
    "\n",
    "train_steps = train_data.__len__()\n",
    "val_steps = val_data.__len__()\n",
    "\n",
    "print(f\"Labels : {label_map}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccd43c-2e82-42b7-8771-6c591817def7",
   "metadata": {},
   "source": [
    "### Model\n",
    "*  we will use ResNet50 as pretrained feature extractor.\n",
    "* On top of it we will add detection and classification head as outputs.\n",
    "\n",
    "<p style=\"text-align:center\"><img src=\"assets/encode.png\" alt=\"yolov1_od\" width=\"1080\"/></p>\n",
    "\n",
    "#### Model Prediction  \n",
    "<p style=\"text-align:center\"><img src=\"assets/pred.png\" alt=\"yolov1_od\" width=\"350\"/></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ac81441-50bc-4313-b005-e065318f3cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def YOLO_r50(gsize=7,classes=None):\n",
    "    base_model = tf.keras.applications.ResNet50(include_top=False,input_shape=(None,None,3))\n",
    "    for l in base_model.layers[:]: \n",
    "        l.trainable = True\n",
    "#     for l in base_model.layers[:143]: #143 81\n",
    "#         l.trainable = False\n",
    "\n",
    "    features=base_model.output\n",
    "    pool=layers.GlobalAveragePooling2D()(features)\n",
    "    \n",
    "    box_params  = 5 #[x,y,w,h,confidence]\n",
    "    total_boxes = 2 #[x,y,w,h,confidence],[x,y,w,h,confidence]\n",
    "        \n",
    "    # Detection head\n",
    "    out_dim  = (box_params*total_boxes)+classes\n",
    "    det_map  = layers.Dense(gsize*gsize*out_dim)(pool)# detection map\n",
    "    \n",
    "    det_out = layers.Reshape((gsize,gsize,out_dim))(det_map)\n",
    "    \n",
    "    mod = models.Model(base_model.input,det_out)\n",
    "    return mod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e5356fe-f8e6-4547-a227-e4e5b467e4c7",
   "metadata": {},
   "source": [
    "#### YOLOv1 Loss Function\n",
    "\n",
    "* Loss function is one of the core part of detection problem, The model can diverge if loss function is not proper and can become unstable during training.\n",
    "* YOLOv1 LOSS function can be split into three parts mainly, In YOLOv1 all these are penalized with squared error loss.\n",
    "  * Bounding Box Loss \n",
    "  * Object Confidence Loss\n",
    "  * Class Loss\n",
    "  \n",
    "  \n",
    "     <p style=\"text-align:center\"><img src=\"assets/loss.jpg\" alt=\"yolov1_od\" width=\"850\"/></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4431d9de-7848-406d-83b6-1f7e3744de1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def yolov1_loss(y_true,y_pred):\n",
    "    \"\"\"[g_cx,g_cy,g_wid,g_hei]\n",
    "    [0,1,2,3,4],[5,6,7,8,9],[10,11]\n",
    "    \"\"\"\n",
    "#     y_true = tf.cast(y_true,tf.float32)\n",
    "#     y_pred = tf.cast(y_pred,tf.float32)\n",
    "    s = 7\n",
    "    classes_count  = 6\n",
    "    lambda_coord = 5.0\n",
    "    lambda_noobj = 0.5\n",
    "    identity_obj = tf.expand_dims(y_true[...,4],axis=-1)\n",
    "    identity_obj = tf.reshape(identity_obj,(-1,s*s,1))  #shape=(N, 49, 1)\n",
    "\n",
    "    # box_loss\n",
    "    true_box = tf.reshape(y_true[...,:4],(-1,s*s,4))\n",
    "    predbox1 = tf.reshape(y_pred[...,:4],(-1,s*s,4))\n",
    "    predbox2 = tf.reshape(y_pred[...,5:9],(-1,s*s,4))\n",
    "    ious = tf.concat((iou(true_box,predbox1)[:,:,tf.newaxis],iou(true_box,predbox2)[:,:,tf.newaxis]),axis=-1)\n",
    "    maxes = tf.expand_dims(tf.cast(tf.argmax(ious,axis=-1),tf.float32),axis=-1)\n",
    "    \n",
    "    true_box_xy = tf.reshape(y_true[...,:2],(-1,s*s,2))\n",
    "    predbox1_xy = tf.reshape(y_pred[...,:2],(-1,s*s,2))\n",
    "    predbox2_xy = tf.reshape(y_pred[...,5:7],(-1,s*s,2))\n",
    "    \n",
    "    box_pred_xy   = identity_obj*(((1-maxes)*predbox1_xy)+(maxes*predbox2_xy))\n",
    "    box_target_xy = identity_obj*true_box_xy\n",
    "    \n",
    "    true_box_wh = tf.reshape(y_true[...,2:4],(-1,s*s,2))\n",
    "    predbox1_wh = tf.reshape(y_pred[...,2:4],(-1,s*s,2))\n",
    "    predbox2_wh = tf.reshape(y_pred[...,7:9],(-1,s*s,2))\n",
    "    \n",
    "    \n",
    "    box_pred_wh   = identity_obj*(((1-maxes)*predbox1_wh)+(maxes*predbox2_wh))\n",
    "    box_target_wh = identity_obj*true_box_wh\n",
    "    \n",
    "    box_pred_wh = tf.sqrt(tf.maximum(box_pred_wh, 1e-6))\n",
    "    box_target_wh = tf.sqrt(tf.maximum(box_target_wh, 1e-6))\n",
    "    \n",
    "    box_wh_loss = tf.losses.mean_squared_error(box_target_wh,box_pred_wh)\n",
    "    box_xy_loss = tf.losses.mean_squared_error(box_target_xy,box_pred_xy)\n",
    "    box_loss = tf.reduce_sum(box_wh_loss+box_xy_loss)\n",
    "\n",
    "    \n",
    "    #object loss\n",
    "    true_obj = tf.reshape(y_true[...,4],(-1,s*s,1))\n",
    "    predobj1 = tf.reshape(y_pred[...,4],(-1,s*s,1))\n",
    "    predobj2 = tf.reshape(y_pred[...,9],(-1,s*s,1))\n",
    "    \n",
    "    obj_box   = (((1-maxes)*predobj1)+(maxes*predobj2))\n",
    "    obj_loss  = tf.keras.losses.mean_squared_error(identity_obj*obj_box,identity_obj*true_obj)\n",
    "    obj_loss  = tf.reduce_sum(obj_loss)\n",
    "    #no-object loss\n",
    "    no_obj_loss1 = tf.keras.losses.mean_squared_error((1-identity_obj)*predobj1,(1-identity_obj)*true_obj)\n",
    "    no_obj_loss2 = tf.keras.losses.mean_squared_error((1-identity_obj)*predobj2,(1-identity_obj)*true_obj)\n",
    "    no_obj_loss  = tf.reduce_sum(no_obj_loss1+no_obj_loss2)\n",
    "\n",
    "    \n",
    "    #class loss\n",
    "    true_class = identity_obj*tf.reshape(y_true[...,5:],(-1,s*s,classes_count))\n",
    "    pred_class = identity_obj*tf.reshape(y_pred[...,10:],(-1,s*s,classes_count))\n",
    "    class_loss = tf.reduce_sum(tf.keras.losses.mean_squared_error(true_class,pred_class))\n",
    "    \n",
    "    final_loss = (box_loss*lambda_coord)+tf.cast(obj_loss,tf.float32)+(lambda_noobj*tf.cast(no_obj_loss,tf.float32))+tf.cast(class_loss,tf.float32)\n",
    "\n",
    "    return final_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a65fee0-9683-476d-8683-8295dfddabea",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "09982a21-2e30-4ea6-8918-c6a4274efa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo_r50 = YOLO_r50(gsize=grid_size,classes=classes)\n",
    "yolo_r50.load_weights(\"checkpoint.h5\")\n",
    "ckpt = ModelCheckpoint(\"checkpoint_001.h5\",monitor=\"val_loss\",save_weights_only=True,save_best_only=True)\n",
    "yolo_r50.compile(optimizer=Adam(5e-4),loss=yolov1_loss)\n",
    "yolo_r50.fit(train_data,batch_size=batch_size,steps_per_epoch=train_steps,epochs=10,\n",
    "            validation_data=val_data,validation_steps=val_steps,callbacks=[ckpt,ReduceLROnPlateau(patience=3,cooldown=1)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d78318-dd69-40cb-9817-df8a7eef812d",
   "metadata": {},
   "source": [
    "### Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7900a10e-1174-4d52-8721-e801127df99a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = \"test_images/6294178496.jpg\"#\"pascal/464_train_scaled/\"+tuple(tfs.sample(1)[\"image\"])[0]\n",
    "output,copy_tensor   = get_predictions(item,model=yolo_r50)\n",
    "boxes,classes,scores = process_predictions(output,confidence=0.5)\n",
    "output = get_output(copy_tensor,boxes,classes,scores,label_map=label_map)\n",
    "cv2.imwrite(\"assets/output.jpg\",output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1888cbb5-603e-41de-b22e-ef097dcda2f4",
   "metadata": {},
   "source": [
    "<p style=\"text-align:center\"><img src=\"assets/output.jpg\" alt=\"yolov1_od\" width=\"450\"/></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12355685-7cf1-4971-930a-bacd52d84d3a",
   "metadata": {},
   "source": [
    "## Conclusion and limitations:\n",
    "\n",
    "* It looks like the YOLOv1 is not great for detecting dense object groups like flock of birds or group of cars. but why?\n",
    "  * The bouding box encoded schema is not great, so it can't detect densly packed objects in the image.\n",
    "* How to overcome the flaws?\n",
    "  * Anchor boxes technique, which are very similar to the above method, but brings in bag of tricks to cover most of the aspect ratios and scales to     detect wide range of  objetcs.\n",
    "* How to classfiy Fundamental hierarchy for object detction systems up till now?\n",
    "  * we can classify them as:\n",
    "    * Single stage detectors -  They use pre-encoded box techniques like YOLO,SSD\n",
    "    * Multi stage detectors -  They use model to guess and propose boxes with respect to the task, like Faster RCNN, they have something called   \n",
    "      RPN(region proposal network which replaces brute force box encoding technique)\n",
    "    * End-to-End  - Using Transformer based networks with set based loss functions we train end-to-end detectors which is most advanced and exciting       research topic."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.5",
   "language": "python",
   "name": "tf2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
