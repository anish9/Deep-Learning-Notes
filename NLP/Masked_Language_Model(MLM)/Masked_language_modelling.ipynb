{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c565ef9b-759a-4147-b583-fbdf544939d2",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange;text-align:center;font-family:courier;font-size:280%\">Masked Language model From Scratch Using Encoder only Transformers</h1>\n",
    "<p style=\"color:orange;text-align:center;font-family:courier\"> The objective is to understand and train a MLM(Masked language model) which can be also called as a type of foundational model.\n",
    "\n",
    "### Objectives \n",
    "* Understand the theory and building blocks of NLP(Natural Language Processing Pipeline.\n",
    "* Generate a basic understanding of how to use Tensorflow Keras API for creating custom utilities(We leverage our pipeline using KerasNLP package).\n",
    "* Simplify the pedagogy of explaining NLP topics and specific to MLM.\n",
    "* As an example we will use **Wikitext dataset** since this is an unsupervised learning task.\n",
    "<!-- * Though the code works there are significant drawbacks with yolov1 which has been addressed on YoloV2,YoloV3 -->\n",
    "\n",
    "\n",
    "<p style=\"text-align:center\"><img src=\"assets/MLM.jpeg\" alt=\"textcla\" width=\"540\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e061b87-1baf-4a48-b3b2-c30e43b32ef6",
   "metadata": {},
   "source": [
    "### Import the dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c813705d-bc8c-4e16-b56a-6dab394fc56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import GPT2TokenizerFast #use BPE tokenizer\n",
    "from tensorflow.keras.layers import Input,Embedding,Dense\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from keras_nlp.layers import TransformerEncoder,SinePositionEncoding\n",
    "\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ad773b-df07-46e6-9bf1-a8efdd83df20",
   "metadata": {},
   "source": [
    "As a beginner if you are barging into NLP it is very crucial to understand tokenizers(Words to Numbers), there are different techniques used to achieve this.\n",
    "\n",
    "Byte-Pair Encoding (BPE) was initially developed as an algorithm to compress texts, and then used by OpenAI for tokenization when pretraining the GPT model. Itâ€™s used by a lot of Transformer models, including GPT, GPT-2, RoBERTa, BART, and DeBERTa. To know more https://huggingface.co/learn/nlp-course/chapter6/5?fw=pt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb8f004-2e7b-4b32-8ef5-c399cfd35555",
   "metadata": {},
   "source": [
    "### Load the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "d0aec3ce-3b92-4430-8c20-7456d53ab2ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mask idx:  50257\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.read_csv(\"wiki001_sample.csv\")\n",
    "text = dataset[\"text\"][:]\n",
    "text = [str(i) for i in text if len(str(i)) >3]\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained(\"gpt2\")\n",
    "vocab_size = tokenizer.vocab_size\n",
    "tokenizer.add_tokens(\" [MASK]\")\n",
    "MASK_TOKEN_IDX = tokenizer(\" [MASK]\").input_ids[0]\n",
    "print(\"Mask idx: \",MASK_TOKEN_IDX)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42f7e5a-21b0-486a-9983-107380aa4922",
   "metadata": {},
   "source": [
    "### Custom DataLoader for MLM task\n",
    "* The objective here is randomly mask words and train the Model to predict the Masked words, in higher level it learns a language.\n",
    "  * Example : INPUT: The dog was [MASK] in the dawn. OUTPUT: The dog was barking in the dawn.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "775131a3-343a-4ca7-ae5f-183a045dde23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLM_dataset(tf.keras.utils.Sequence):\n",
    "    def __init__(self,text_list,eos_text,max_token_length,batch_size,tokenizer,shuffle=True):\n",
    "        self.textlist = text_list\n",
    "        self.shuffle = shuffle\n",
    "        self.batchsize = batch_size\n",
    "        self.max_token_length = max_token_length\n",
    "        self.eos_text = eos_text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.indices = list(range(len(self.textlist)))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return int(len(self.indices)/self.batchsize)\n",
    "    \n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indices)\n",
    "            \n",
    "    \n",
    "    def prepare_sample(self,idx):\n",
    "        sample = self.tokenizer(self.textlist[idx])\n",
    "        \n",
    "        tokens = [sample.input_ids]\n",
    "        pad_tokens = pad_sequences(tokens,maxlen=self.max_token_length,padding=\"post\",truncating=\"post\",value=self.eos_text)[0]\n",
    "        weights = np.zeros((self.max_token_length))\n",
    "        output_tokens = pad_tokens.copy()\n",
    "        choices = [np.random.randint(3,18,3)]\n",
    "        random_mask_indices = random.choice(choices)\n",
    "        pad_tokens[random_mask_indices] = MASK_TOKEN_IDX\n",
    "        weights[random_mask_indices] = 1\n",
    "        return pad_tokens,output_tokens,weights\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        masked_inp = []\n",
    "        unmasked_out = []\n",
    "        weights = []\n",
    "        \n",
    "        batch_list = list(range(idx * self.batchsize,(idx + 1)*self.batchsize))\n",
    "        for idx_ in batch_list:\n",
    "            x,y,z = self.prepare_sample(idx_)\n",
    "            masked_inp.append(x)\n",
    "            unmasked_out.append(y)\n",
    "            weights.append(z)\n",
    "            \n",
    "        return tf.cast(masked_inp,tf.int32),tf.cast(unmasked_out,tf.int32),tf.cast(weights,tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61d9185-614f-4783-9d85-55032ad490ab",
   "metadata": {},
   "source": [
    "### Create dataset pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "515b5a1f-e25f-4fb1-b689-16e9498a5538",
   "metadata": {},
   "outputs": [],
   "source": [
    "EOS_text = tokenizer.encode(\"<|endoftext|>\")[0] #from gpt2 vocab\n",
    "max_seq_length = 32 #Max token length\n",
    "batch = 32\n",
    "\n",
    "mlmdataset = MLM_dataset(text_list=text,batch_size=batch,\n",
    "                         eos_text=EOS_text,max_token_length=max_seq_length,\n",
    "                         tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a494058f-e6dd-47ee-81b6-56181d5b98bf",
   "metadata": {},
   "source": [
    "### Build the model and set the model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "350f3786-1c28-4227-808f-e66f2f1e0aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 256\n",
    "INTERMEDIATE_DIM = 512\n",
    "NUM_BLOCKS = 6\n",
    "HEADS=6\n",
    "\n",
    "def mlm_nn():\n",
    "    vocab_size = MASK_TOKEN_IDX+3\n",
    "    embedding_dim=EMBEDDING_DIM\n",
    "    inputs = Input((None,), dtype=tf.int32)\n",
    "    embedding = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
    "    positional_encoding = SinePositionEncoding()(embedding)\n",
    "    outputs = embedding + positional_encoding\n",
    "    for i in range(NUM_BLOCKS):\n",
    "        encoder = TransformerEncoder(intermediate_dim=INTERMEDIATE_DIM,num_heads=HEADS,activation=\"gelu\")(outputs)\n",
    "    \n",
    "    output_node = Dense(vocab_size)(encoder)\n",
    "    model = tf.keras.models.Model(inputs,output_node)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1718fa-79a3-47f7-9fe2-c051ebaa1886",
   "metadata": {},
   "source": [
    "### Model initialization and hyper-parameters setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "81770e24-3040-416c-aca4-a2c40a6168fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = mlm_nn()\n",
    "epochs = 100\n",
    "loss_func = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "optimizer = tf.keras.optimizers.Adam(1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c983d055-9ffd-437e-bb31-45fac9c8aa0a",
   "metadata": {},
   "source": [
    "### Train the model - (custom training loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e718fab-abe7-43ba-88ae-a5f20ee31f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(epochs):\n",
    "    epoch_loss= 0\n",
    "    print(\"EPOCH : \",i)\n",
    "    for idx,(x,y,z) in enumerate(mlmdataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            nn_out = nn(x)\n",
    "            loss_ = loss_func(y,nn_out,sample_weight=z)\n",
    "            epoch_loss+=loss_\n",
    "        trainable_vars = nn.trainable_variables\n",
    "        gradients = tape.gradient(loss_, trainable_vars)\n",
    "        optimizer.apply_gradients(zip(gradients,trainable_vars))\n",
    "        if idx%100==0:\n",
    "            print(f\"loss : {loss_.numpy()}\")\n",
    "            nn.save_weights(\"savednn.h5\")\n",
    "    epoch_loss_value = epoch_loss/int(len(text)/24)\n",
    "    print(f\"epochloss : {epoch_loss_value.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f726f8-a9f9-430f-8bd7-fa6413e65f3a",
   "metadata": {},
   "source": [
    "### An experimental training was already executed, will utilize that for direct inference\n",
    "* Load the trained weights for inference **wiki_base256.h5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "078e3b1e-0f59-4405-94aa-9b44466cbbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.load_weights(\"wiki_base256.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6a6ac502-e431-4bd4-a8e3-0340b7bc89cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_masked_word(text):\n",
    "    tokenized = tf.cast(tokenizer.encode(text),tf.int32)\n",
    "    tokenized = tokenized[tf.newaxis,:].numpy()\n",
    "    ELEMENTS = np.where(tokenized[0]==MASK_TOKEN_IDX)[0]\n",
    "    indices = tf.argmax(nn.predict(tokenized,verbose=0),axis=-1)[0]\n",
    "    for i in ELEMENTS:\n",
    "        tokenized[0][i] =  indices[i]\n",
    "    return tokenizer.decode(tokenized[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331dd47b-70e2-45ba-a8c3-ad14979e251e",
   "metadata": {},
   "source": [
    "### Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7f2ef31f-fa4a-4c6c-b0af-b5cabc3a8391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT: The revolution of russian army was [MASK] by civiliztion of\n",
      "OUTPUT: The revolution of russian army was captured by civiliztion of\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: the dog was [MASK] in the kennel\n",
      "OUTPUT: the dog was placed in the kennel\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: A business proposal is a written offer from a [MASK] to a prospective sponsor\n",
      "OUTPUT: A business proposal is a written offer from a reference to a prospective sponsor\n",
      "--------------------------------------------------------------------------------\n",
      "INPUT: the law states we can never fight [MASK] our own customs\n",
      "OUTPUT: the law states we can never fight against our own customs\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "test_examples = [\"The revolution of russian army was [MASK] by civiliztion of\",\n",
    "                 \"the dog was [MASK] in the kennel\",\n",
    "                 \"A business proposal is a written offer from a [MASK] to a prospective sponsor\",\n",
    "                 \"the law states we can never fight [MASK] our own customs\"]\n",
    "for i in test_examples:\n",
    "    print(f\"INPUT: {i}\")\n",
    "    print(f\"OUTPUT: {predict_masked_word(i)}\")\n",
    "    print(\"-\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.9",
   "language": "python",
   "name": "tf2.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
