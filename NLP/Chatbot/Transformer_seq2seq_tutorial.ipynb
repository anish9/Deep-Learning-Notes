{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7713974-8d20-4453-b3d3-b8f5688866e1",
   "metadata": {},
   "source": [
    "<h1 style=\"color:orange;text-align:center;font-family:courier;font-size:280%\">Chatbot From Scratch Using Transformers(seq-to-seq)</h1>\n",
    "<p style=\"color:orange;text-align:center;font-family:courier\"> The objective is to understand how to build a seq-to-seq model from scratch to build a interactive chatbot</p>\n",
    "\n",
    "### Objectives \n",
    "* Understand the theory and building blocks of NLP(Natural Language Processing Pipeline.\n",
    "* Generate a basic understanding of how to use Tensorflow Keras API for creating custom utilities.\n",
    "* Simplify the pedagogy of explaining NLP topics especially encoder decoder models.\n",
    "* As an example we will use **Dialog generation** Dataset for our interactive chatbot.\n",
    "<!-- * Though the code works there are significant drawbacks with yolov1 which has been addressed on YoloV2,YoloV3 -->\n",
    "\n",
    "\n",
    "<p style=\"text-align:center\"><img src=\"assets/Chatbot.png\" alt=\"textcla\" width=\"540\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1edd05b-4c1c-442a-bae9-8538ffe6ac57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "from utils import *\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d24bfea8-db61-401f-af6b-75edbee6a8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH=25\n",
    "BATCH_SIZE = 14\n",
    "BUFFER_SIZE = 2000\n",
    "EMBEDDING_DIM=512\n",
    "DENSE_DIM=1024\n",
    "HEADS=4\n",
    "EPOCHS=50\n",
    "AUTO = tf.data.AUTOTUNE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abec7ecd-4e2a-4d32-9654-d85fc3c4d442",
   "metadata": {},
   "source": [
    "### Loading data and pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdac4141-e9a1-42ab-a3c4-d3c7033f5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"dataset_conver.csv\",nrows=4000)\n",
    "source = \"question\"\n",
    "target = \"answer\"\n",
    "dataset[source] = dataset[source].map(preprocess_text)\n",
    "dataset[target] = dataset[target].map(lambda x:preprocess_text(x,block=\"target\"))\n",
    "dataset = dataset.drop([2],axis=0)\n",
    "dataset.reset_index(drop=True,inplace=True)\n",
    "\n",
    "question = [dataset[source].tolist()[j].split() for j in range(dataset.shape[0])]\n",
    "question =sum(question,[])\n",
    "\n",
    "answers = [dataset[target].tolist()[j].split() for j in range(dataset.shape[0])]\n",
    "answers =sum(answers,[])\n",
    "\n",
    "\n",
    "source_vocab = len(list(set(question)))\n",
    "target_vocab = len(list(set(answers)))\n",
    "\n",
    "source_vectorizer = layers.TextVectorization(max_tokens=source_vocab,output_mode=\"int\")\n",
    "source_vectorizer.adapt(dataset[source])\n",
    "target_vectorizer = layers.TextVectorization(max_tokens=target_vocab,output_mode=\"int\")\n",
    "target_vectorizer.adapt(dataset[target])\n",
    "\n",
    "train,val = train_test_split(dataset,test_size=0.1,shuffle=True)\n",
    "source_train_data,target_train_data = dataset_prep(source_vectorizer(train[source]),\n",
    "                                                    target_vectorizer(train[target]),maxlen=MAX_LENGTH)\n",
    "source_val_data,target_val_data = dataset_prep(source_vectorizer(val[source]),\n",
    "                                               target_vectorizer(val[target]),maxlen=MAX_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1900738-0572-4526-966e-33f14bcc48e7",
   "metadata": {},
   "source": [
    "### Process train and validation for the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1145bb31-df85-4fb9-a24b-c11c0650a794",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_tensor_slices((source_train_data,target_train_data))\n",
    "train_set = train_set.map(pipeline,num_parallel_calls=AUTO).prefetch(10)\n",
    "train_set = train_set.batch(BATCH_SIZE,drop_remainder=True).shuffle(BUFFER_SIZE)\n",
    "\n",
    "val_set = tf.data.Dataset.from_tensor_slices((source_val_data,target_val_data))\n",
    "val_set = val_set.map(pipeline,num_parallel_calls=AUTO).prefetch(10)\n",
    "val_set = val_set.batch(BATCH_SIZE,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa5c2f-f9d4-4753-bfcf-17300e01b77c",
   "metadata": {},
   "source": [
    "### Building layers\n",
    "<p style=\"text-align:center\"><img src=\"assets/arch.jpg\" alt=\"textcla\" width=\"240\"/>\n",
    "\n",
    "* The following components are required to build the above architecture:\n",
    "    * Positional Encoding.\n",
    "    * Transformer Encoder.\n",
    "    * Transformer Decoder.\n",
    "\n",
    "For learning about the internal mechanisms these layers follow this <a href=https://medium.com/@joshanish/dissecting-transformers-part1-2df55e234b9a>blogpost</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "444dae28-3776-41c4-8df4-70e4892391e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Postional_Encoding(tf.keras.layers.Layer):\n",
    "    def __init__(self,embedding_depth,vocab,sequence_length):\n",
    "        super(Postional_Encoding,self).__init__()\n",
    "        self.embedding_depth = embedding_depth\n",
    "        self.sequence_length = sequence_length\n",
    "        self.embed = tf.keras.layers.Embedding(vocab,embedding_depth)\n",
    "        \n",
    "    def call(self,data):\n",
    "        batch_dim = tf.shape(data)[0]\n",
    "        embeds = np.arange(self.embedding_depth)[np.newaxis,:]\n",
    "        embeds = 1 / np.power(10000, (2 * (embeds//2)) / np.float32(self.embedding_depth))\n",
    "        location_id = np.arange(self.sequence_length)[:,np.newaxis]\n",
    "        pos = embeds*location_id\n",
    "        pos[:,::2] = np.sin(pos[:,::2])\n",
    "        pos[:,1::2] = np.cos(pos[:,1::2])\n",
    "        pos = tf.tile(pos[tf.newaxis,:,:],(batch_dim,1,1))\n",
    "        pos = tf.cast(pos,tf.float32)\n",
    "        embed = self.embed(data)\n",
    "        return embed+pos \n",
    "\n",
    "    def compute_mask(self,data,mask=None):\n",
    "        return tf.not_equal(0,data)\n",
    "    \n",
    "    \n",
    "class Transformer_Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self,embedding_depth,dense_dim,heads=2,**kwargs):\n",
    "        super(Transformer_Encoder,self).__init__(**kwargs)\n",
    "        self.attention = tf.keras.layers.MultiHeadAttention(num_heads=heads,key_dim=embedding_depth)\n",
    "        self.dense_proj = tf.keras.Sequential([tf.keras.layers.Dense(dense_dim, activation=\"relu\"), \n",
    "                                               tf.keras.layers.Dense(embedding_depth),])\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "        \n",
    "    def call(self,x,mask=None):\n",
    "        padding_mask = tf.cast(mask,tf.int64)[:,:,tf.newaxis]\n",
    "        attention_out = self.attention(x,x,x,attention_mask=padding_mask)\n",
    "        layernorm1 = self.layernorm_1(attention_out+x)\n",
    "        denseproj  = self.dense_proj(layernorm1)\n",
    "        layernorm2 = self.layernorm_2(denseproj+layernorm1)\n",
    "        return layernorm2\n",
    "\n",
    "    \n",
    "class Transformer_Decoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, latent_dim, num_heads, **kwargs):\n",
    "        super(Transformer_Decoder, self).__init__(**kwargs)\n",
    "        self.attention_1 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = tf.keras.Sequential(\n",
    "            [layers.Dense(latent_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_2 = tf.keras.layers.LayerNormalization()\n",
    "        self.layernorm_3 = tf.keras.layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.generate_causal_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(query=inputs, value=inputs, key=inputs, attention_mask=causal_mask)\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        attention_output_2,att_weights = self.attention_2(query=out_1,value=encoder_outputs,key=encoder_outputs,attention_mask=padding_mask,\n",
    "                                                         return_attention_scores=True)\n",
    "        out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output),att_weights\n",
    "    \n",
    "    def generate_causal_mask(self,inputs):\n",
    "        batch_size,seq_length = tf.shape(inputs)[0],tf.shape(inputs)[1]\n",
    "        x = tf.range(seq_length)\n",
    "        y = tf.range(seq_length)[:,tf.newaxis]\n",
    "        causal_mask = tf.cast(y>=x,dtype=\"int32\")[tf.newaxis,:,:]\n",
    "        causal_mask = tf.tile(causal_mask,(batch_size,1,1))\n",
    "        return causal_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd528c00-124d-42e3-86a8-7c88116b3300",
   "metadata": {},
   "source": [
    "### Chaining our layers to build our complete model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71af0714-f256-4d20-94c8-272f73eb8089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Transformer_Model(EMBEDDING_DEPTH,DENSE,VOCAB,LENGTH,HEADS=2):\n",
    "    encoder_inp = layers.Input(shape=(None,),dtype=tf.int32,name=\"encoder_input\")\n",
    "    decoder_inp = layers.Input(shape=(None,),dtype=tf.int32,name=\"decoder_input\")\n",
    "    encoder_pos_embed = Postional_Encoding(EMBEDDING_DEPTH,VOCAB,LENGTH)(encoder_inp)\n",
    "    encoder_attention1 = Transformer_Encoder(EMBEDDING_DEPTH,DENSE,heads=HEADS)(encoder_pos_embed)\n",
    "    \n",
    "    decoder_pos_embed = Postional_Encoding(EMBEDDING_DEPTH,VOCAB,LENGTH)(decoder_inp)\n",
    "    decoder_attention1,_ = Transformer_Decoder(EMBEDDING_DEPTH,DENSE,HEADS)(decoder_pos_embed,encoder_attention1)\n",
    "    decoder_attention2,attention_weights = Transformer_Decoder(EMBEDDING_DEPTH,DENSE,HEADS)(decoder_attention1,encoder_attention1)\n",
    "    decoder_attention2 = layers.Dropout(0.3)(decoder_attention2)\n",
    "    output = tf.keras.layers.Dense(VOCAB,activation=\"softmax\")(decoder_attention2)\n",
    "\n",
    "    model = tf.keras.Model((encoder_inp,decoder_inp),output)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a6ab72e-ed42-499b-b97e-d6c494a387b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      [(None, None)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "postional__encoding (Postional_ (None, 25, 512)      1997312     encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "postional__encoding_1 (Postiona (None, 25, 512)      1997312     decoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "transformer__encoder (Transform (None, 25, 512)      5253120     postional__encoding[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "transformer__decoder (Transform ((None, 25, 512), (N 9455104     postional__encoding_1[0][0]      \n",
      "                                                                 transformer__encoder[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "transformer__decoder_1 (Transfo ((None, 25, 512), (N 9455104     transformer__decoder[0][0]       \n",
      "                                                                 transformer__encoder[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 25, 512)      0           transformer__decoder_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_6 (Dense)                 (None, 25, 3901)     2001213     dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 30,159,165\n",
      "Trainable params: 30,159,165\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Epoch 1/50\n",
      "257/257 [==============================] - 10s 22ms/step - loss: 2.2336 - accuracy: 0.1285 - val_loss: 2.2355 - val_accuracy: 0.1297\n",
      "Epoch 2/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 2.0463 - accuracy: 0.2037 - val_loss: 2.0731 - val_accuracy: 0.2192\n",
      "Epoch 3/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 2.0018 - accuracy: 0.2188 - val_loss: 2.0472 - val_accuracy: 0.2253\n",
      "Epoch 4/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.9614 - accuracy: 0.2201 - val_loss: 2.0357 - val_accuracy: 0.2194\n",
      "Epoch 5/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.9140 - accuracy: 0.2226 - val_loss: 2.0130 - val_accuracy: 0.2200\n",
      "Epoch 6/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.8761 - accuracy: 0.2255 - val_loss: 2.0087 - val_accuracy: 0.2253\n",
      "Epoch 7/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.8441 - accuracy: 0.2305 - val_loss: 2.0025 - val_accuracy: 0.2245\n",
      "Epoch 8/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.8144 - accuracy: 0.2331 - val_loss: 2.0002 - val_accuracy: 0.2253\n",
      "Epoch 9/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.7904 - accuracy: 0.2368 - val_loss: 1.9962 - val_accuracy: 0.2250\n",
      "Epoch 10/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.7684 - accuracy: 0.2400 - val_loss: 1.9998 - val_accuracy: 0.2263\n",
      "Epoch 11/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.7468 - accuracy: 0.2440 - val_loss: 1.9960 - val_accuracy: 0.2332\n",
      "Epoch 12/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.7262 - accuracy: 0.2476 - val_loss: 1.9983 - val_accuracy: 0.2364\n",
      "Epoch 13/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.7056 - accuracy: 0.2512 - val_loss: 2.0032 - val_accuracy: 0.2417\n",
      "Epoch 14/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.6852 - accuracy: 0.2539 - val_loss: 2.0025 - val_accuracy: 0.2425\n",
      "Epoch 15/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.6658 - accuracy: 0.2575 - val_loss: 2.0145 - val_accuracy: 0.2258\n",
      "Epoch 16/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.6466 - accuracy: 0.2608 - val_loss: 2.0165 - val_accuracy: 0.2295\n",
      "Epoch 17/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.6241 - accuracy: 0.2645 - val_loss: 2.0089 - val_accuracy: 0.2427\n",
      "Epoch 18/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.6049 - accuracy: 0.2661 - val_loss: 2.0227 - val_accuracy: 0.2327\n",
      "Epoch 19/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.5787 - accuracy: 0.2726 - val_loss: 2.0061 - val_accuracy: 0.2401\n",
      "Epoch 20/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.5574 - accuracy: 0.2751 - val_loss: 2.0142 - val_accuracy: 0.2340\n",
      "Epoch 21/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.5373 - accuracy: 0.2757 - val_loss: 2.0234 - val_accuracy: 0.2385\n",
      "Epoch 22/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.5131 - accuracy: 0.2801 - val_loss: 2.0099 - val_accuracy: 0.2446\n",
      "Epoch 23/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.4827 - accuracy: 0.2873 - val_loss: 2.0285 - val_accuracy: 0.2332\n",
      "Epoch 24/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.4596 - accuracy: 0.2891 - val_loss: 2.0286 - val_accuracy: 0.2446\n",
      "Epoch 25/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.4324 - accuracy: 0.2953 - val_loss: 2.0287 - val_accuracy: 0.2406\n",
      "Epoch 26/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.4046 - accuracy: 0.3014 - val_loss: 2.0362 - val_accuracy: 0.2419\n",
      "Epoch 27/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.3786 - accuracy: 0.3066 - val_loss: 2.0432 - val_accuracy: 0.2435\n",
      "Epoch 28/50\n",
      "257/257 [==============================] - 6s 21ms/step - loss: 1.3537 - accuracy: 0.3111 - val_loss: 2.0476 - val_accuracy: 0.2491\n",
      "Epoch 29/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.3239 - accuracy: 0.3193 - val_loss: 2.0734 - val_accuracy: 0.2263\n",
      "Epoch 30/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.2965 - accuracy: 0.3262 - val_loss: 2.0767 - val_accuracy: 0.2388\n",
      "Epoch 31/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.2691 - accuracy: 0.3323 - val_loss: 2.0804 - val_accuracy: 0.2276\n",
      "Epoch 32/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.2403 - accuracy: 0.3415 - val_loss: 2.0792 - val_accuracy: 0.2350\n",
      "Epoch 33/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.2156 - accuracy: 0.3497 - val_loss: 2.0979 - val_accuracy: 0.2372\n",
      "Epoch 34/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.1808 - accuracy: 0.3625 - val_loss: 2.1114 - val_accuracy: 0.2372\n",
      "Epoch 35/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.1491 - accuracy: 0.3762 - val_loss: 2.1231 - val_accuracy: 0.2356\n",
      "Epoch 36/50\n",
      "257/257 [==============================] - 5s 20ms/step - loss: 1.1193 - accuracy: 0.3853 - val_loss: 2.1500 - val_accuracy: 0.2165\n",
      "Epoch 37/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 1.0865 - accuracy: 0.4015 - val_loss: 2.1422 - val_accuracy: 0.2271\n",
      "Epoch 38/50\n",
      "257/257 [==============================] - 5s 20ms/step - loss: 1.0570 - accuracy: 0.4140 - val_loss: 2.1585 - val_accuracy: 0.2218\n",
      "Epoch 39/50\n",
      "257/257 [==============================] - 5s 20ms/step - loss: 1.0204 - accuracy: 0.4314 - val_loss: 2.1735 - val_accuracy: 0.2226\n",
      "Epoch 40/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 0.9924 - accuracy: 0.4447 - val_loss: 2.1865 - val_accuracy: 0.2215\n",
      "Epoch 41/50\n",
      "257/257 [==============================] - 5s 21ms/step - loss: 0.9541 - accuracy: 0.4630 - val_loss: 2.2132 - val_accuracy: 0.2104\n",
      "Epoch 42/50\n",
      "257/257 [==============================] - 5s 20ms/step - loss: 0.9275 - accuracy: 0.4749 - val_loss: 2.2197 - val_accuracy: 0.2231\n",
      "Epoch 43/50\n",
      "257/257 [==============================] - 5s 20ms/step - loss: 0.8905 - accuracy: 0.4954 - val_loss: 2.2413 - val_accuracy: 0.2210\n",
      "Epoch 44/50\n",
      "257/257 [==============================] - 5s 20ms/step - loss: 0.8552 - accuracy: 0.5124 - val_loss: 2.2607 - val_accuracy: 0.2271\n",
      "Epoch 45/50\n",
      "257/257 [==============================] - 5s 20ms/step - loss: 0.8232 - accuracy: 0.5282 - val_loss: 2.2777 - val_accuracy: 0.2218\n",
      "Epoch 46/50\n",
      "257/257 [==============================] - 5s 20ms/step - loss: 0.7927 - accuracy: 0.5443 - val_loss: 2.3194 - val_accuracy: 0.2080\n",
      "Epoch 47/50\n",
      "257/257 [==============================] - 5s 20ms/step - loss: 0.7566 - accuracy: 0.5680 - val_loss: 2.3283 - val_accuracy: 0.2046\n",
      "Epoch 48/50\n",
      "257/257 [==============================] - 5s 20ms/step - loss: 0.7246 - accuracy: 0.5858 - val_loss: 2.3695 - val_accuracy: 0.2186\n",
      "Epoch 49/50\n",
      "257/257 [==============================] - 5s 20ms/step - loss: 0.6943 - accuracy: 0.6026 - val_loss: 2.3792 - val_accuracy: 0.2107\n",
      "Epoch 50/50\n",
      "257/257 [==============================] - 6s 21ms/step - loss: 0.6652 - accuracy: 0.6187 - val_loss: 2.4090 - val_accuracy: 0.2046\n"
     ]
    }
   ],
   "source": [
    "vocab = len(target_vectorizer.get_vocabulary())\n",
    "transformer = Transformer_Model(EMBEDDING_DEPTH=EMBEDDING_DIM,\n",
    "                                DENSE=DENSE_DIM,VOCAB=vocab,\n",
    "                                HEADS=HEADS,LENGTH=MAX_LENGTH,)\n",
    "\n",
    "transformer.summary()\n",
    "transformer.compile(\n",
    "    tf.keras.optimizers.Adam(1e-4), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]\n",
    ")\n",
    "transformer.fit(train_set, epochs=EPOCHS,validation_data=val_set)\n",
    "transformer.save_weights(\"model1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9e1a593-6b4c-414e-89f9-f74de672d972",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "Accuracy is not a right metric to check the progress of a seq-to-seq model unless the model is restricted to very limited intents. we have something called BLEU score to evaluate how meaningful the responses are from the model, The following explanation by Andrew NG explains it well.<br>\n",
    "https://www.youtube.com/watch?v=9ZvTxChwg9A&list=PL1w8k37X_6L_s4ncq-swTBvKDWnRSrinI&index=29"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b0daa7-d32e-4dea-af57-5125484c6a5e",
   "metadata": {},
   "source": [
    "### Building our inference pipeline,\n",
    "Yes, we can chat now !!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "53823af4-629b-48e5-ac53-c28a71235bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"a stranger followed me into a dark tunnel\",\"I was having a good time\",\"his friend acted stupid at that very situation\",\"are you funny\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f9fa4db3-a04b-46b8-8062-40792f9b5db8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Question : a stranger followed me into a dark tunnel\n",
      "Response : i think you dont know that easy but you did\n",
      "--------------------------------------------------------------------------------\n",
      " Question : i was having a good time\n",
      "Response : you know what you did\n",
      "--------------------------------------------------------------------------------\n",
      " Question : his friend acted stupid at that very situation\n",
      "Response : i dont know what the world is he are with the night\n",
      "--------------------------------------------------------------------------------\n",
      " Question : are you funny\n",
      "Response : no no i thought you want to say\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for input_text in questions:\n",
    "\n",
    "    input_text = input_text\n",
    "    input_text = preprocess_text(input_text)\n",
    "\n",
    "    print(f\" Question : {input_text}\")\n",
    "    vectorize_sentence = source_vectorizer([input_text])\n",
    "    encode_sent = tf.keras.preprocessing.sequence.pad_sequences(vectorize_sentence,\n",
    "                                                                       maxlen=MAX_LENGTH,padding=\"post\")\n",
    "\n",
    "    output_seq = \" \"\n",
    "    starter,ender = \"sos\",\"eos\"\n",
    "    output_seq+=starter\n",
    "    tar_vocab =target_vectorizer.get_vocabulary()\n",
    "    target_inv = dict(zip(range(len(tar_vocab)),tar_vocab))\n",
    "\n",
    "    for i in range(MAX_LENGTH):\n",
    "        current = output_seq\n",
    "        decode_sent = tf.keras.preprocessing.sequence.pad_sequences(target_vectorizer([current]),\n",
    "                                                      maxlen=MAX_LENGTH,padding=\"post\")\n",
    "        pred = transformer.predict((encode_sent,decode_sent))\n",
    "\n",
    "        word =target_inv[np.argmax(pred[0,i,:])]\n",
    "\n",
    "\n",
    "        if word == \"eos\":\n",
    "            break\n",
    "        output_seq+=\" \"+word\n",
    "    response = \" \".join(output_seq.split()[1:])\n",
    "    print(f\"Response : {response}\")\n",
    "    print(\"-\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f8b40-47d7-40f5-9954-fb1b0357080c",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "As we can see the chatbot is somewhat able to build some conversation, it can be tuned based on different datasets and different hyperparameters settings to make it much more applicable for real world applications. I assume; I have almost covered most critical parts in building this end-to-end pipeline helping readers to discover more details on NLP world."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2.5",
   "language": "python",
   "name": "tf2.5"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
